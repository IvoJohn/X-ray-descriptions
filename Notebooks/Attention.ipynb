{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XrayNLP4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCqXfi4rR457"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "random_state = 42\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch4NBsEjR8AO"
      },
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.index_to_string = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.string_to_index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_to_string)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        #returns tokenized sentence in a form of list with single words\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "    \n",
        "    def return_index_to_string(self):\n",
        "        return self.index_to_string\n",
        "    \n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "        \n",
        "        for word in self.tokenizer(sentence_list):\n",
        "            if word not in frequencies:\n",
        "                frequencies[word] = 1\n",
        "            else:\n",
        "                frequencies[word] += 1\n",
        "            if frequencies[word] == self.freq_threshold:\n",
        "                self.string_to_index[word] = idx\n",
        "                self.index_to_string[idx] = word\n",
        "                idx += 1\n",
        "                \n",
        "    def numericalize(self, text):\n",
        "        #it takes a sentence and returns indexes of words in it as a form of list\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        \n",
        "        return [self.string_to_index[token] if token in self.string_to_index \n",
        "                else self.string_to_index[\"<UNK>\"] for token in tokenized_text]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juE4xPrSSBMc"
      },
      "source": [
        "class MyCollate:\n",
        "        def __init__(self, pad_idx):\n",
        "            self.pad_idx = pad_idx\n",
        "            \n",
        "        def __call__(self, batch):\n",
        "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "            imgs = torch.cat(imgs, dim=0)\n",
        "            targets = [item[1] for item in batch]\n",
        "            targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
        "            \n",
        "            return imgs, targets    \n",
        "        \n",
        "#here previously batch_first was set to false, but i changed it to true"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-O3L1G-SCa8"
      },
      "source": [
        "class XRayDataset(Dataset):\n",
        "    def __init__(self, cvs_file, path, transform, freq_threshold, size=(624,512)):\n",
        "        #path is for general folder, csv_file is csv file not path\n",
        "        self.path = path\n",
        "        self.dataframe = cvs_file\n",
        "        self.size = size\n",
        "        self.transform = transform\n",
        "        self.freq_thresh = freq_threshold\n",
        "        \n",
        "        self.img_col = self.dataframe[\"Imgs_paths\"]\n",
        "        self.findings_col = self.dataframe[\"findings\"]\n",
        "        \n",
        "        self.vocab = Vocabulary(self.freq_thresh)\n",
        "        self.st = \"\"\n",
        "        self.vocab.build_vocabulary(self.st.join(self.findings_col.tolist()[:]))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        finding = self.findings_col[index]\n",
        "        img_id = self.img_col[index]\n",
        "        img_path = self.path + \"/Images/\" + img_id + \".png\"\n",
        "        img = Image.open(img_path).resize(self.size)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        numericalized_caption = [self.vocab.string_to_index[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(finding)\n",
        "        numericalized_caption.append(self.vocab.string_to_index[\"<EOS>\"])\n",
        "        \n",
        "        return img, torch.tensor(numericalized_caption)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAkeMmBWSDW7"
      },
      "source": [
        "def get_loader(csv_file, path, transform, batch_size, freq_threshold, shuffle=True):\n",
        "    dataset = XRayDataset(csv_file, path, transform, freq_threshold)\n",
        "    \n",
        "    pad_idx = dataset.vocab.string_to_index[\"<PAD>\"]\n",
        "    \n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = shuffle,\n",
        "        collate_fn = MyCollate(pad_idx=pad_idx)        \n",
        "    )\n",
        "\n",
        "    return loader, dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi0htXM1SEUh"
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
        "        for param in inception.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        modules = list(inception.children())[:-4]\n",
        "        self.inception = nn.Sequential(*modules).to(device)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        features = self.inception(images)\n",
        "        #2048\n",
        "        \n",
        "        batch, feature_maps, size_1, size_2 = features.size()\n",
        "        features = features.permute(0,2,3,1)\n",
        "        features = features.view(batch, size_1*size_2, feature_maps) \n",
        "        \n",
        "        return features"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzphL_ksSFoF"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, output_dim=1):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.num_features = num_features #2048\n",
        "        self.hidden_dim = hidden_dim #512\n",
        "        self.output_dim = output_dim #1\n",
        "        \n",
        "        #this layer learns attention over features from encoder\n",
        "        self.W_a = nn.Linear(self.num_features, self.hidden_dim).to(device)\n",
        "        #this layers learns attention over previous decoder state\n",
        "        self.U_a = nn.Linear(self.hidden_dim, self.hidden_dim).to(device)\n",
        "        #this produces the output from two previous ones\n",
        "        self.V_a = nn.Linear(self.hidden_dim, self.output_dim).to(device)\n",
        "        \n",
        "    def forward(self, features, decoder_hidden):\n",
        "        #features are from encoder cnn\n",
        "        #decoder_hidden is previous hidden state from decoder lstm\n",
        "        \n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1) #to add time steps/batch\n",
        "        Wa = self.W_a(features) #calculating attn over features from encoder\n",
        "        Ua = self.U_a(decoder_hidden) #calculating attention over previous hidden state\n",
        "        atten_tan = torch.tanh(Wa+Ua) #calculating tangent from added two previous tensors\n",
        "        atten_score = self.V_a(atten_tan) #linear layer to calculate attention weights\n",
        "        atten_weight = F.softmax(atten_score, dim=1) #activation to this layer\n",
        "        \n",
        "        context = torch.sum(atten_weight*features, dim=1) #multiplication of features and atten weights\n",
        "        #print(\"Atten weight before squeeze\",atten_weight.size())\n",
        "        atten_weight = atten_weight.squeeze(dim=2)\n",
        "        #print(\"Atten weight after squeeze dim=2\", atten_weight.size())\n",
        "        \n",
        "        return context, atten_weight"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgtrhqDJSGqw"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_features, embedding_dim, hidden_dim, vocab_size, p=0.5):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.num_features = num_features #2048\n",
        "        self.embedding_dim = embedding_dim #256\n",
        "        self.hidden_dim = hidden_dim #512\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sample_temp = 0.5\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim).to(device) #embedding of caption's words\n",
        "        self.lstm = nn.LSTMCell(embedding_dim + num_features, hidden_dim).to(device) #concat vector of embedded hidden state and \n",
        "        #context vector from attention\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size).to(device)\n",
        "        \n",
        "        self.attention = BahdanauAttention(self.num_features, self.hidden_dim)\n",
        "        self.dropout = nn.Dropout(p=p)\n",
        "        #initialization to hidden state and cell memory\n",
        "        self.init_h = nn.Linear(num_features, hidden_dim).to(device)\n",
        "        self.init_c = nn.Linear(num_features, hidden_dim).to(device)\n",
        "        \n",
        "    def forward(self, captions, features, sample_prob = 0.0):\n",
        "        \n",
        "        embed = self.embeddings(captions) #should be of size batch_size, sequence_length, embedding_dimension\n",
        "        #print(\"Size of embed: \", embed.size())\n",
        "        h, c = self.init_hidden(features)\n",
        "        #print(\"Size of h, c: \", h.size(), c.size())\n",
        "        sequence_len = captions.size(1)\n",
        "        feature_size = features.size(1)\n",
        "        batch_size = features.size(0)\n",
        "        #print(\"Size of features: \", features.size())\n",
        "        outputs = torch.zeros(batch_size, sequence_len, self.vocab_size).to(device)\n",
        "        atten_weights = torch.zeros(batch_size, sequence_len, feature_size).to(device)\n",
        "        \n",
        "        for t in range(sequence_len): #loop for each word in caption\n",
        "            sample_prob = 0.0 if t == 0 else 0.5\n",
        "            use_sampling = np.random.random() < sample_prob\n",
        "            if use_sampling == False:\n",
        "                word_embed = embed[:,t,:] #embedding from current time step\n",
        "            context, atten_weight = self.attention(features, h)\n",
        "            input_concat = torch.cat([word_embed, context], 1) #input to lstm\n",
        "            h, c = self.lstm(input_concat, (h,c))\n",
        "            h = self.dropout(h)\n",
        "            output = self.fc(h)\n",
        "            if use_sampling == True:\n",
        "                scaled_output = output / self.sample_temp\n",
        "                scoring = F.log_softmax(scaled_output, dim=1)\n",
        "                top_idx = scoring.topk(1)[1]\n",
        "                word_embed = self.embeddings(top_idx).squeeze(1) #word predicted previously\n",
        "            outputs[:,t,:] = output\n",
        "            atten_weights[:,t,:] = atten_weight\n",
        "        return outputs, atten_weights\n",
        "    \n",
        "    def init_hidden(self, features):\n",
        "        mean_annotations = torch.mean(features, dim=1)\n",
        "        h0 = self.init_h(mean_annotations)\n",
        "        c0 = self.init_c(mean_annotations)\n",
        "        return h0, c0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2CmPYNiSJM-"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryowcz0cSKRz"
      },
      "source": [
        "def train(epoch, encoder, decoder, optimizer, criterion, total_step, num_epochs, data_loader, write_file=None, save_every=None):\n",
        "    #function for a single epoch\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    \n",
        "    for i_step in range(1, total_step+1):\n",
        "        encoder.eval()\n",
        "        decoder.train()\n",
        "        \n",
        "        images, captions = next(iter(data_loader)) #this should obtain a whole batch of images and captions for them\n",
        "        captions_target = captions[:,1:].to(device) #without start token\n",
        "        captions_train = captions[:,:-1].to(device) #lstm must predict end token\n",
        "\n",
        "        img = images.to(device)\n",
        "\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        features = encoder(img)\n",
        "        outputs, atten_weights = decoder(captions=captions_train, features = features)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions_target.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        stats = 'Epoch train: [%d/%d], Step train: [%d/%d], Loss train: %.4f' % (epoch, num_epochs, i_step, total_step, loss.item())\n",
        "        print('\\r' + stats, end=\"\")\n",
        "            \n",
        "    epoch_loss_avg = epoch_loss / total_step\n",
        "    \n",
        "    print('\\r')\n",
        "    print('Epoch train:', epoch)\n",
        "    print('\\r' + 'Avg. Loss train: %.4f, Avg.' % (epoch_loss_avg), end=\"\")\n",
        "    print('\\r')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llProobkSLPp"
      },
      "source": [
        "path = '/content/drive/MyDrive/Data/XrayNLP'\n",
        "csv_path = path + \"/\" + \"dataframe.csv\"\n",
        "checkpoint_path = path + \"/\" + \"checkpoint3.pth.tar\"\n",
        "\n",
        "batch_size = 32\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_features = 2048\n",
        "n_epochs = 5\n",
        "freq_threshold = 3\n",
        "learning_rate = 0.001\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "csv_train, csv_validate, csv_test = np.split(df.sample(frac=1, random_state=random_state), [int(.8*len(df)), int(.9*len(df))])\n",
        "csv_train, csv_validate, csv_test = csv_train.reset_index(), csv_validate.reset_index(), csv_test.reset_index()\n",
        "\n",
        "_, dataset = get_loader(df, path, transform, batch_size, freq_threshold)\n",
        "train_loader, train_dataset = get_loader(csv_train, path, transform, batch_size, freq_threshold)\n",
        "valid_loader, valid_dataset = get_loader(csv_validate, path, transform, batch_size, freq_threshold)\n",
        "test_loader, test_dataset = get_loader(csv_test, path, transform, batch_size, freq_threshold)\n",
        "\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "total_step = len(train_dataset)//batch_size + 1\n",
        "\n",
        "encoder = EncoderCNN()\n",
        "decoder = DecoderRNN(num_features, embed_size, hidden_size, vocab_size)\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.string_to_index[\"<PAD>\"])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeNsPIL1T38k",
        "outputId": "3bcf4193-de1d-4bb7-d972-7565872758f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(0, n_epochs):\n",
        "  train(i, encoder, decoder, optimizer, criterion, total_step, n_epochs, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch train: [0/5], Step train: [189/189], Loss train: 3.3317\n",
            "Epoch train: 0\n",
            "Avg. Loss train: 4.1910, Avg.\n",
            "Epoch train: [1/5], Step train: [189/189], Loss train: 3.3377\n",
            "Epoch train: 1\n",
            "Avg. Loss train: 3.5527, Avg.\n",
            "Epoch train: [2/5], Step train: [95/189], Loss train: 3.4425"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}