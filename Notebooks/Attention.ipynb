{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XrayNLP4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99cbc768e22c4dbeb34585847d840645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8b9c704f959c46529d608fa40b5a5fbe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2fbea141f42e4e7091d6c5876495c3bf",
              "IPY_MODEL_d89c3a2df55845bfb2e99147dba1f4bc"
            ]
          }
        },
        "8b9c704f959c46529d608fa40b5a5fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2fbea141f42e4e7091d6c5876495c3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1f257095d31474aa8d36d11183824c0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108949747,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108949747,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c09418a7a6d4b0c84deb35e97d08353"
          }
        },
        "d89c3a2df55845bfb2e99147dba1f4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0a2ea41108c4d418611741bf3a82f83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:24&lt;00:00, 4.52MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c8abc1b52cd4537bdb835930551c022"
          }
        },
        "d1f257095d31474aa8d36d11183824c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c09418a7a6d4b0c84deb35e97d08353": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0a2ea41108c4d418611741bf3a82f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c8abc1b52cd4537bdb835930551c022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCqXfi4rR457"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "random_state = 42\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch4NBsEjR8AO"
      },
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.index_to_string = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.string_to_index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_to_string)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        #returns tokenized sentence in a form of list with single words\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "    \n",
        "    def return_index_to_string(self):\n",
        "        return self.index_to_string\n",
        "    \n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "        \n",
        "        for word in self.tokenizer(sentence_list):\n",
        "            if word not in frequencies:\n",
        "                frequencies[word] = 1\n",
        "            else:\n",
        "                frequencies[word] += 1\n",
        "            if frequencies[word] == self.freq_threshold:\n",
        "                self.string_to_index[word] = idx\n",
        "                self.index_to_string[idx] = word\n",
        "                idx += 1\n",
        "                \n",
        "    def numericalize(self, text):\n",
        "        #it takes a sentence and returns indexes of words in it as a form of list\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        \n",
        "        return [self.string_to_index[token] if token in self.string_to_index \n",
        "                else self.string_to_index[\"<UNK>\"] for token in tokenized_text]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juE4xPrSSBMc"
      },
      "source": [
        "class MyCollate:\n",
        "        def __init__(self, pad_idx):\n",
        "            self.pad_idx = pad_idx\n",
        "            \n",
        "        def __call__(self, batch):\n",
        "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "            imgs = torch.cat(imgs, dim=0)\n",
        "            targets = [item[1] for item in batch]\n",
        "            targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
        "            \n",
        "            return imgs, targets    \n",
        "        \n",
        "#here previously batch_first was set to false, but i changed it to true"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-O3L1G-SCa8"
      },
      "source": [
        "class XRayDataset(Dataset):\n",
        "    def __init__(self, cvs_file, path, transform, freq_threshold, size=(624,512)):\n",
        "        #path is for general folder, csv_file is csv file not path\n",
        "        self.path = path\n",
        "        self.dataframe = cvs_file\n",
        "        self.size = size\n",
        "        self.transform = transform\n",
        "        self.freq_thresh = freq_threshold\n",
        "        \n",
        "        self.img_col = self.dataframe[\"Imgs_paths\"]\n",
        "        self.findings_col = self.dataframe[\"findings\"]\n",
        "        \n",
        "        self.vocab = Vocabulary(self.freq_thresh)\n",
        "        self.st = \"\"\n",
        "        self.vocab.build_vocabulary(self.st.join(self.findings_col.tolist()[:]))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        finding = self.findings_col[index]\n",
        "        img_id = self.img_col[index]\n",
        "        img_path = self.path + \"/Images/\" + img_id + \".png\"\n",
        "        img = Image.open(img_path).resize(self.size)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        numericalized_caption = [self.vocab.string_to_index[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(finding)\n",
        "        numericalized_caption.append(self.vocab.string_to_index[\"<EOS>\"])\n",
        "        \n",
        "        return img, torch.tensor(numericalized_caption)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAkeMmBWSDW7"
      },
      "source": [
        "def get_loader(csv_file, path, transform, batch_size, freq_threshold, shuffle=True):\n",
        "    dataset = XRayDataset(csv_file, path, transform, freq_threshold)\n",
        "    \n",
        "    pad_idx = dataset.vocab.string_to_index[\"<PAD>\"]\n",
        "    \n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = shuffle,\n",
        "        collate_fn = MyCollate(pad_idx=pad_idx)        \n",
        "    )\n",
        "\n",
        "    return loader, dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi0htXM1SEUh"
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
        "        for param in inception.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        modules = list(inception.children())[:-4]\n",
        "        self.inception = nn.Sequential(*modules).to(device)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        features = self.inception(images)\n",
        "        #2048\n",
        "        \n",
        "        batch, feature_maps, size_1, size_2 = features.size()\n",
        "        features = features.permute(0,2,3,1)\n",
        "        features = features.view(batch, size_1*size_2, feature_maps) \n",
        "        \n",
        "        return features"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzphL_ksSFoF"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, output_dim=1):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.num_features = num_features #2048\n",
        "        self.hidden_dim = hidden_dim #512\n",
        "        self.output_dim = output_dim #1\n",
        "        \n",
        "        #this layer learns attention over features from encoder\n",
        "        self.W_a = nn.Linear(self.num_features, self.hidden_dim).to(device)\n",
        "        #this layers learns attention over previous decoder state\n",
        "        self.U_a = nn.Linear(self.hidden_dim, self.hidden_dim).to(device)\n",
        "        #this produces the output from two previous ones\n",
        "        self.V_a = nn.Linear(self.hidden_dim, self.output_dim).to(device)\n",
        "        \n",
        "    def forward(self, features, decoder_hidden):\n",
        "        #features are from encoder cnn\n",
        "        #decoder_hidden is previous hidden state from decoder lstm\n",
        "        \n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1) #to add time steps/batch\n",
        "        Wa = self.W_a(features) #calculating attn over features from encoder\n",
        "        Ua = self.U_a(decoder_hidden) #calculating attention over previous hidden state\n",
        "        atten_tan = torch.tanh(Wa+Ua) #calculating tangent from added two previous tensors\n",
        "        atten_score = self.V_a(atten_tan) #linear layer to calculate attention weights\n",
        "        atten_weight = F.softmax(atten_score, dim=1) #activation to this layer\n",
        "        \n",
        "        context = torch.sum(atten_weight*features, dim=1) #multiplication of features and atten weights\n",
        "        #print(\"Atten weight before squeeze\",atten_weight.size())\n",
        "        atten_weight = atten_weight.squeeze(dim=2)\n",
        "        #print(\"Atten weight after squeeze dim=2\", atten_weight.size())\n",
        "        \n",
        "        return context, atten_weight"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgtrhqDJSGqw"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_features, embedding_dim, hidden_dim, vocab_size, p=0.5):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.num_features = num_features #2048\n",
        "        self.embedding_dim = embedding_dim #256\n",
        "        self.hidden_dim = hidden_dim #512\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sample_temp = 0.5\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim).to(device) #embedding of caption's words\n",
        "        self.lstm = nn.LSTMCell(embedding_dim + num_features, hidden_dim).to(device) #concat vector of embedded hidden state and \n",
        "        #context vector from attention\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size).to(device)\n",
        "        \n",
        "        self.attention = BahdanauAttention(self.num_features, self.hidden_dim)\n",
        "        self.dropout = nn.Dropout(p=p)\n",
        "        #initialization to hidden state and cell memory\n",
        "        self.init_h = nn.Linear(num_features, hidden_dim).to(device)\n",
        "        self.init_c = nn.Linear(num_features, hidden_dim).to(device)\n",
        "        \n",
        "    def forward(self, captions, features, sample_prob = 0.0):\n",
        "        \n",
        "        embed = self.embeddings(captions) #should be of size batch_size, sequence_length, embedding_dimension\n",
        "        #print(\"Size of embed: \", embed.size())\n",
        "        h, c = self.init_hidden(features)\n",
        "        #print(\"Size of h, c: \", h.size(), c.size())\n",
        "        sequence_len = captions.size(1)\n",
        "        feature_size = features.size(1)\n",
        "        batch_size = features.size(0)\n",
        "        #print(\"Size of features: \", features.size())\n",
        "        outputs = torch.zeros(batch_size, sequence_len, self.vocab_size).to(device)\n",
        "        atten_weights = torch.zeros(batch_size, sequence_len, feature_size).to(device)\n",
        "        \n",
        "        for t in range(sequence_len): #loop for each word in caption\n",
        "            sample_prob = 0.0 if t == 0 else 0.5\n",
        "            use_sampling = np.random.random() < sample_prob\n",
        "            if use_sampling == False:\n",
        "                word_embed = embed[:,t,:] #embedding from current time step\n",
        "            context, atten_weight = self.attention(features, h)\n",
        "            input_concat = torch.cat([word_embed, context], 1) #input to lstm\n",
        "            h, c = self.lstm(input_concat, (h,c))\n",
        "            h = self.dropout(h)\n",
        "            output = self.fc(h)\n",
        "            if use_sampling == True:\n",
        "                scaled_output = output / self.sample_temp\n",
        "                scoring = F.log_softmax(scaled_output, dim=1)\n",
        "                top_idx = scoring.topk(1)[1]\n",
        "                word_embed = self.embeddings(top_idx).squeeze(1) #word predicted previously\n",
        "            outputs[:,t,:] = output\n",
        "            atten_weights[:,t,:] = atten_weight\n",
        "        return outputs, atten_weights\n",
        "    \n",
        "    def init_hidden(self, features):\n",
        "        mean_annotations = torch.mean(features, dim=1)\n",
        "        h0 = self.init_h(mean_annotations)\n",
        "        c0 = self.init_c(mean_annotations)\n",
        "        return h0, c0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2CmPYNiSJM-"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v20jOK7PU0ES"
      },
      "source": [
        "def save_checkopoint(state, file_path):\n",
        "  print(\"Saving\")\n",
        "  torch.save(state, file_path)\n",
        "  print(\"Saved\")\n",
        "\n",
        "def train(n_epochs, encoder, decoder, optimizer, criterion, batch_size, data_loader, valid_loader, checkpoint_path = None, save_model=True):\n",
        "\n",
        "  loss_history = []\n",
        "  valid_loss_history = []\n",
        "\n",
        "  total_step = len(data_loader)//batch_size + 1\n",
        "  total_valid_step = len(valid_loader)//batch_size + 1\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_valid_loss = 0.0\n",
        "    \n",
        "    #training loop\n",
        "    for i_step in range(1, total_step):\n",
        "        encoder.eval()\n",
        "        decoder.train()\n",
        "        \n",
        "        images, captions = next(iter(data_loader)) #this should obtain a whole batch of images and captions for them\n",
        "        captions_target = captions[:,1:].to(device) #without start token\n",
        "        captions_train = captions[:,:-1].to(device) #lstm must predict end token\n",
        "\n",
        "        img = images.to(device)\n",
        "\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        features = encoder(img)\n",
        "        outputs, atten_weights = decoder(captions=captions_train, features = features)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions_target.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        stats = 'Epoch train: [%d/%d], Batch train: [%d/%d], Loss train: %.4f' % (epoch, n_epochs, i_step, total_step, loss.item())\n",
        "        print('\\r' + stats, end=\"\")\n",
        "\n",
        "      #validation loop\n",
        "    for i_step_val in range(1, total_valid_step):\n",
        "        decoder.eval()\n",
        "\n",
        "        images, captions = next(iter(valid_loader))\n",
        "        captions_target = captions[:,1:].to(device)\n",
        "        captions_valid = captions[:,:-1].to(device)\n",
        "        \n",
        "        img = images.to(device)\n",
        "\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        features = encoder(img)\n",
        "        outputs, atten_weights = decoder(captions=captions_valid, features=features)\n",
        "\n",
        "        loss_val = criterion(outputs.view(-1, vocab_size), captions_valid.reshape(-1))\n",
        "        epoch_valid_loss += loss_val\n",
        "            \n",
        "    epoch_loss_avg = epoch_loss / total_step\n",
        "    epoch_valid_loss_avg = epoch_valid_loss / total_valid_step\n",
        "    loss_history.append(epoch_loss_avg)\n",
        "    valid_loss_history.append(epoch_valid_loss_avg)\n",
        "      \n",
        "    print('\\r')\n",
        "    print('Epoch train:', epoch)\n",
        "    print('\\r' + 'Avg. Loss train: %.4f, Avg.' % (epoch_loss_avg), end=\"\")\n",
        "    print('\\r')\n",
        "\n",
        "    checkpoint = {'state_dict': decoder.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    if save_model:\n",
        "      save_checkopoint(checkpoint, checkpoint_path)\n",
        "\n",
        "  return loss_history, valid_loss_history"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryowcz0cSKRz"
      },
      "source": [
        "\"\"\"def train(epoch, encoder, decoder, optimizer, criterion, total_step, num_epochs, data_loader, write_file=None, save_every=None):\n",
        "    #function for a single epoch\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    \n",
        "    for i_step in range(1, total_step+1):\n",
        "        encoder.eval()\n",
        "        decoder.train()\n",
        "        \n",
        "        images, captions = next(iter(data_loader)) #this should obtain a whole batch of images and captions for them\n",
        "        captions_target = captions[:,1:].to(device) #without start token\n",
        "        captions_train = captions[:,:-1].to(device) #lstm must predict end token\n",
        "\n",
        "        img = images.to(device)\n",
        "\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        features = encoder(img)\n",
        "        outputs, atten_weights = decoder(captions=captions_train, features = features)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions_target.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        stats = 'Epoch train: [%d/%d], Step train: [%d/%d], Loss train: %.4f' % (epoch, num_epochs, i_step, total_step, loss.item())\n",
        "        print('\\r' + stats, end=\"\")\n",
        "            \n",
        "    epoch_loss_avg = epoch_loss / total_step\n",
        "    \n",
        "    print('\\r')\n",
        "    print('Epoch train:', epoch)\n",
        "    print('\\r' + 'Avg. Loss train: %.4f, Avg.' % (epoch_loss_avg), end=\"\")\n",
        "    print('\\r')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "99cbc768e22c4dbeb34585847d840645",
            "8b9c704f959c46529d608fa40b5a5fbe",
            "2fbea141f42e4e7091d6c5876495c3bf",
            "d89c3a2df55845bfb2e99147dba1f4bc",
            "d1f257095d31474aa8d36d11183824c0",
            "4c09418a7a6d4b0c84deb35e97d08353",
            "d0a2ea41108c4d418611741bf3a82f83",
            "5c8abc1b52cd4537bdb835930551c022"
          ]
        },
        "id": "llProobkSLPp",
        "outputId": "98a1003c-3afc-4522-cd96-1273235e794a"
      },
      "source": [
        "path = '/content/drive/MyDrive/Data/XrayNLP'\n",
        "csv_path = path + \"/\" + \"dataframe.csv\"\n",
        "checkpoint_path = path + \"/\" + \"attention1.pth.tar\"\n",
        "\n",
        "batch_size = 32\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_features = 2048\n",
        "n_epochs = 5\n",
        "freq_threshold = 3\n",
        "learning_rate = 0.001\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "csv_train, csv_validate, csv_test = np.split(df.sample(frac=1, random_state=random_state), [int(.8*len(df)), int(.9*len(df))])\n",
        "csv_train, csv_validate, csv_test = csv_train.reset_index(), csv_validate.reset_index(), csv_test.reset_index()\n",
        "\n",
        "_, dataset = get_loader(df, path, transform, batch_size, freq_threshold)\n",
        "train_loader, train_dataset = get_loader(csv_train, path, transform, batch_size, freq_threshold)\n",
        "valid_loader, valid_dataset = get_loader(csv_validate, path, transform, batch_size, freq_threshold)\n",
        "test_loader, test_dataset = get_loader(csv_test, path, transform, batch_size, freq_threshold)\n",
        "\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "#total_step = len(train_dataset)//batch_size + 1\n",
        "\n",
        "encoder = EncoderCNN()\n",
        "decoder = DecoderRNN(num_features, embed_size, hidden_size, vocab_size)\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.string_to_index[\"<PAD>\"])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99cbc768e22c4dbeb34585847d840645",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108949747.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHVvs9PIVzIx",
        "outputId": "d7dd9aec-5786-4d9d-ae9c-b5b1de8d7c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss_history, valid_loss_history = train(n_epochs, encoder, decoder, optimizer, criterion, batch_size, train_loader, valid_loader, checkpoint_path)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch train: [0/5], Batch train: [5/6], Loss train: 4.5836\n",
            "Epoch train: 0\n",
            "Avg. Loss train: 4.0113, Avg.\n",
            "Saving\n",
            "Saved\n",
            "Epoch train: [1/5], Batch train: [5/6], Loss train: 4.6062\n",
            "Epoch train: 1\n",
            "Avg. Loss train: 3.9158, Avg.\n",
            "Saving\n",
            "Saved\n",
            "Epoch train: [2/5], Batch train: [5/6], Loss train: 4.5171\n",
            "Epoch train: 2\n",
            "Avg. Loss train: 3.9088, Avg.\n",
            "Saving\n",
            "Saved\n",
            "Epoch train: [3/5], Batch train: [5/6], Loss train: 4.5243\n",
            "Epoch train: 3\n",
            "Avg. Loss train: 3.7007, Avg.\n",
            "Saving\n",
            "Saved\n",
            "Epoch train: [4/5], Batch train: [5/6], Loss train: 4.2242\n",
            "Epoch train: 4\n",
            "Avg. Loss train: 3.6639, Avg.\n",
            "Saving\n",
            "Saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}