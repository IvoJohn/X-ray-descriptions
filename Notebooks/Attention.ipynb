{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random_state = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batches goes first in all places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.index_to_string = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.string_to_index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_to_string)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        #returns tokenized sentence in a form of list with single words\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def return_index_to_string(self):\n",
    "        return self.index_to_string\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for word in self.tokenizer(sentence_list):\n",
    "            if word not in frequencies:\n",
    "                frequencies[word] = 1\n",
    "            else:\n",
    "                frequencies[word] += 1\n",
    "            if frequencies[word] == self.freq_threshold:\n",
    "                self.string_to_index[word] = idx\n",
    "                self.index_to_string[idx] = word\n",
    "                idx += 1\n",
    "                \n",
    "    def numericalize(self, text):\n",
    "        #it takes a sentence and returns indexes of words in it as a form of list\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        \n",
    "        return [self.string_to_index[token] if token in self.string_to_index \n",
    "                else self.string_to_index[\"<UNK>\"] for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "        def __init__(self, pad_idx):\n",
    "            self.pad_idx = pad_idx\n",
    "            \n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "            targets = [item[1] for item in batch]\n",
    "            targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "            \n",
    "            return imgs, targets    \n",
    "        \n",
    "#here previously batch_first was set to false, but i changed it to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, cvs_file, path, transform, freq_threshold, size=(624,512)):\n",
    "        #path is for general folder, csv_file is csv file not path\n",
    "        self.path = path\n",
    "        self.dataframe = cvs_file\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_col = self.dataframe[\"Imgs_paths\"]\n",
    "        self.findings_col = self.dataframe[\"findings\"]\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_thresh)\n",
    "        self.st = \"\"\n",
    "        self.vocab.build_vocabulary(self.st.join(self.findings_col.tolist()[:]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        findings = self.findings_col[index]\n",
    "        img_id = self.img_col[index]\n",
    "        img_path = self.path + \"/Images/\" + img_id + \".png\"\n",
    "        img = Image.open(img_path).resize(self.size)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        numericalized_caption = [self.vocab.string_to_index[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(finding)\n",
    "        numericalized_caption.append(self.vocab.string_to_index[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(csv_file, path, transform, batch_size, shuffle=True):\n",
    "    dataset = XRayDataset(csv_file, path, transform)\n",
    "    \n",
    "    pad_idx = dataset.vocab.string_to_index[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        collate_fn = MyCollate(pad_idx=pad_idx)        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        for param in inception.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        modules = list(inception.children())[:-4]\n",
    "        self.inception = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        #2048\n",
    "        \n",
    "        batch, feature_maps, size_1, size_2 = features.size()\n",
    "        features = features.permute(0,2,3,1)\n",
    "        features = features.view(batch, size_1*size_2, feature_maps) \n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, output_dim=1):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.num_features = num_features #2048\n",
    "        self.hidden_dim = hidden_dim #512\n",
    "        self.output_dim = output_dim #1\n",
    "        \n",
    "        #this layer learns attention over features from encoder\n",
    "        self.W_a = nn.Linear(self.num_features, self.hidden_dim)\n",
    "        #this layers learns attention over previous decoder state\n",
    "        self.U_a = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        #this produces the output from two previous ones\n",
    "        self.V_a = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, features, decoder_hidden):\n",
    "        #features are from encoder cnn\n",
    "        #decoder_hidden is previous hidden state from decoder lstm\n",
    "        \n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1) #to add time steps/batch\n",
    "        Wa = self.W_a(features) #calculating attn over features from encoder\n",
    "        Ua = self.U_a(decoder_hidden) #calculating attention over previous hidden state\n",
    "        atten_tan = torch.tanh(Wa+Ua) #calculating tangent from added two previous tensors\n",
    "        atten_score = self.V_a(atten_tan) #linear layer to calculate attention weights\n",
    "        atten_weight = F.softmax(atten_score, dim=1) #activation to this layer\n",
    "        \n",
    "        context = torch.sum(atten_weight*features, dim=1) #multiplication of features and atten weights\n",
    "        print(\"Atten weight before squeeze\",atten_weight.size())\n",
    "        atten_weight = atten_weight.squeeze(dim=2)\n",
    "        print(\"Atten weight after squeeze dim=2\", atten_weight.size())\n",
    "        \n",
    "        return context, atten_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
