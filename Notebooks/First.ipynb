{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pvguFR5v8eR4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from google.colab import output\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJf7jgYBD89r"
   },
   "source": [
    "## Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZuH7_fz8h8K"
   },
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "path = ''\n",
    "imgs_path = path + '/Images'\n",
    "reports_path = path + '/reports'\n",
    "imgs_list = os.listdir(imgs_path)\n",
    "rep_list = os.listdir(reports_path)\n",
    "\n",
    "print(\"Number of images: \", len(imgs_list))\n",
    "print(\"Number of reports: \", len(rep_list))\n",
    "\n",
    "n_imgs_per_report = []\n",
    "for rep in rep_list:\n",
    "    tree = ET.parse(reports_path + \"/\" + rep)\n",
    "    root = tree.getroot()\n",
    "    images = root.findall(\".//parentImage\")\n",
    "    n_images = len(images)\n",
    "    n_imgs_per_report.append(n_images)\n",
    "    \n",
    "print(\"Number of images per report: \", Counter(n_imgs_per_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLhWBXE98kUB"
   },
   "outputs": [],
   "source": [
    "indications = []\n",
    "findings = []\n",
    "imgs_paths = []\n",
    "\n",
    "for rep in rep_list:\n",
    "    root = ET.parse(reports_path + \"/\" + rep).getroot()\n",
    "    imgs = root.findall(\".//parentImage\")\n",
    "    n_images = len(imgs)\n",
    "    if n_images == 0:\n",
    "        continue\n",
    "    else:\n",
    "        description = root.findall(\".//AbstractText\")\n",
    "        indication = description[1].text\n",
    "        finding = \"\"\n",
    "        for d in description[2:]:\n",
    "            try:\n",
    "                finding += \". \" + d.text\n",
    "            except:\n",
    "                pass\n",
    "        indications.append(indication)\n",
    "        indications.append(indication)\n",
    "        findings.append(finding)\n",
    "        findings.append(finding)\n",
    "\n",
    "        if n_images >= 2:\n",
    "            imgs_paths.append(imgs[0].attrib['id'])\n",
    "            imgs_paths.append(imgs[1].attrib['id'])\n",
    "        if n_images == 1:\n",
    "            imgs_paths.append(imgs[0].attrib['id'])\n",
    "            imgs_paths.append(imgs[0].attrib['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si0MCMZd8t6T"
   },
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uXLioQ4e8qmr"
   },
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_thresh):\n",
    "        self.index_to_string = {0: \"<PAD>\", 1: \"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.string_to_index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_thresh = freq_thresh\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_to_string)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def return_index_to_string(self):\n",
    "        return self.index_to_string\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        \"\"\"for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                \n",
    "                if frequencies[word] == self.freq_thresh:\n",
    "                    self.string_to_index[word] = idx\n",
    "                    self.index_to_string[idx] = word\n",
    "                    idx += 1\"\"\"\n",
    "        for word in self.tokenizer(sentence_list):\n",
    "            if word not in frequencies:\n",
    "                frequencies[word] = 1\n",
    "            else:\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "            if frequencies[word] == self.freq_thresh:\n",
    "                self.string_to_index[word] = idx\n",
    "                self.index_to_string[idx] = word\n",
    "                idx += 1\n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        \n",
    "        return [self.string_to_index[token] if token in self.string_to_index else self.string_to_index[\"<UNK>\"] \n",
    "                for token in tokenized_text]\n",
    "    \n",
    "class MyCollate:\n",
    "        def __init__(self, pad_idx):\n",
    "            self.pad_idx = pad_idx\n",
    "            \n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "            targets = [item[1] for item in batch]\n",
    "            targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "            \n",
    "            return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qfUIlmp48wP9"
   },
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, csv_file, path,transform, freq_thresh=3, size=(624,512)):\n",
    "        #path is for general folder, csv_file is this file\n",
    "        self.path = path\n",
    "        self.dataframe = csv_file\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_col = self.dataframe[\"Imgs_paths\"]\n",
    "        self.findings_col = self.dataframe[\"findings\"]\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_thresh)\n",
    "        self.st = \"\"\n",
    "        self.vocab.build_vocabulary(self.st.join(self.findings_col.tolist()[:]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) #7702\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        finding = self.findings_col[index]\n",
    "        img_id = self.img_col[index]\n",
    "        img_path = self.path + \"/Images/\" + img_id + \".png\"\n",
    "        #img = Image.open(img_path).convert('L').resize(self.size) this will be used when I addapt model to grayscale imgs\n",
    "        img = Image.open(img_path).resize(self.size)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.string_to_index[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(finding)\n",
    "        numericalized_caption.append(self.vocab.string_to_index[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZHcxpVHf8yHg"
   },
   "outputs": [],
   "source": [
    "def get_loader(csv_file, path, transform, batch_size=32, shuffle=True):\n",
    "    dataset = XRayDataset(csv_file, path, transform)\n",
    "    \n",
    "    pad_idx = dataset.vocab.string_to_index[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "    )\n",
    "    \n",
    "    return loader, dataset\n",
    "\n",
    "transform = transforms.Compose(\n",
    "[\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42Dq4kTd6bTc"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "pM7qdfK26cFf"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False) #zmienic ten model pozniej\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        \n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN\n",
    "                \n",
    "        return self.dropout(self.relu(features))\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption_image(self, image, vocabulary, max_length=50): #here max length shoudl be changed\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                \n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "                \n",
    "                if vocabulary.index_to_string[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "        return [vocabulary.index_to_string[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "dtU0kWcy6cnu"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_path):\n",
    "    print(\"Saving\")\n",
    "    torch.save(state, file_path)\n",
    "\n",
    "def train(embed_size, hidden_size, vocab_size, num_layers, learning_rate, num_epochs, model, criterion, optimizer,train_loader,valid_loader, device, file_path):\n",
    "    transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299,299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    loss_history = []\n",
    "    valid_loss_history = []\n",
    "\n",
    "    load_model = False\n",
    "    save_model = True\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "        model.train()\n",
    "\n",
    "        # train set\n",
    "        epoch_train_loss = []\n",
    "        for batch_idx, (imgs, captions) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            outputs = model(imgs, captions[:-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            epoch_train_loss.append(loss)\n",
    "            optimizer.step()\n",
    "            \n",
    "            output.clear()\n",
    "            print(\"Example {}/{}. Epoch {}/{}. Loss: {}\".format(batch_idx, len(train_loader), epoch+1, num_epochs, loss))\n",
    "            #if batch_idx >= 2:\n",
    "            #    break\n",
    "        if save_model:\n",
    "              save_checkpoint(checkpoint, file_path)\n",
    "        loss_history.append(np.sum(epoch_train_loss)/(batch_idx+1)) #tutaj sie chyba dodaje tylko ostatnia strata a nie srednia z epoki\n",
    "\n",
    "        # validation set\n",
    "        model.eval()\n",
    "        epoch_valid_loss = []\n",
    "        for batch_idx, (imgs, captions) in enumerate(valid_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            outputs = model(imgs, captions[:-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "            epoch_valid_loss.append(loss)\n",
    "            step += 1\n",
    "\n",
    "            output.clear()\n",
    "            print(\"Valid example {}/{}. Epoch {}/{}. Loss: {}\".format(batch_idx, len(train_loader), epoch+1, num_epochs, loss))\n",
    "            #if batch_idx >= 2:\n",
    "            #    break\n",
    "        valid_loss_history.append(np.sum(epoch_valid_loss)/(batch_idx+1))\n",
    "\n",
    "    return loss_history, valid_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzmnvgUD6irh",
    "outputId": "14646679-31da-4ad9-ac6e-948d97aff392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid example 23/189. Epoch 5/5. Loss: 6.08713960647583\n"
     ]
    }
   ],
   "source": [
    "path = '/content/drive/MyDrive/Data/XrayNLP'\n",
    "csv_path = path + \"/\" + \"dataframe.csv\"\n",
    "checkpoint_path = path + \"/\" + \"checkpoint.pth.tar\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "csv_train, csv_validate, csv_test = np.split(df.sample(frac=1, random_state=random_state), [int(.8*len(df)), int(.9*len(df))])\n",
    "csv_train, csv_validate, csv_test = csv_train.reset_index(), csv_validate.reset_index(), csv_test.reset_index()\n",
    "\n",
    "_, dataset = get_loader(df, path, transform)\n",
    "train_loader, train_dataset = get_loader(csv_train, path, transform)\n",
    "valid_loader, valid_dataset = get_loader(csv_validate, path, transform)\n",
    "test_loader, test_dataset = get_loader(csv_test, path, transform)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "num_layers = 1\n",
    "learning_rate = 3e-4\n",
    "num_epochs=5\n",
    "    \n",
    "step = 0\n",
    "    \n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.string_to_index[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history, valid_loss_history = train(embed_size, hidden_size, vocab_size, num_layers, learning_rate, num_epochs, model, criterion, optimizer,train_loader,valid_loader, device, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnCHjAbOTaI",
    "outputId": "0b045100-ae82-482b-dd93-15c323394e5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(4.6280, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(3.2495, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(2.8621, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(2.6376, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(2.4853, device='cuda:0', grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hrnk5jIVOVVp",
    "outputId": "5983dbd7-cf6f-47a5-b213-ca51381be288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5.1149, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(5.4642, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(5.7283, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(5.9285, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(6.0927, device='cuda:0', grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUgP4mxFFD2Q"
   },
   "source": [
    "Loss at the beginning: 7.2701191902160645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "nlvsqxLQCyGu"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "      for batch_idx, (imgs, captions) in enumerate(test_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            outputs = model(imgs, captions[:-1])\n",
    "            outputs_list.append(outputs)            \n",
    "            #writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "            if batch_idx >= 1:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6-OefLUG3q4",
    "outputId": "92b5e027-9762-4979-9773-ed41b17e44ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102, 1478])"
      ]
     },
     "execution_count": 180,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(outputs_list[0][:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "0z_m0YM9N3Sd",
    "outputId": "881627af-77fc-4814-d459-63cc31580a63"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' <SOS> . xxxx silhouette of soft size xxxx airspace pneumothorax limits no . normal . of normal . . is 2 there pneumothorax airspace 2 . is focal right . evidence disease density . xxxx opacity size size size size size are . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict = Vocabulary.return_index_to_string(dataset.vocab)\n",
    "\n",
    "result = \"\"\n",
    "for value in np.argmax(outputs_list[1][:,20,:].tolist(), axis=1):\n",
    "    result += \" \" + index_dict[value]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RvNZGeCx46A5"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/Data/XrayNLP'\n",
    "csv_path = path + \"/\" + \"dataframe.csv\"\n",
    "checkpoint_path = path + \"/\" + \"checkpoint.pth.tar\"\n",
    "\n",
    "train_loader, dataset = get_loader(csv_path, path, transform)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 1\n",
    "learning_rate = 3e-4\n",
    "num_epochs=2\n",
    "    \n",
    "step = 0\n",
    "\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.string_to_index[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tXHXdL0N2SCX"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path):\n",
    "    print(\"Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bz8stWXx5T3e",
    "outputId": "f623aeff-3a32-40e9-aecd-16824b814098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "XrayNLP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
